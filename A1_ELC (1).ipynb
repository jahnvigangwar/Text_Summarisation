{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Develop a language-independent extractive text summarization system that generates a summary for a given document**"
      ],
      "metadata": {
        "id": "97wbyZQy0kUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Libraries: \n",
        "\n",
        "NLTK is a platform for building Python programs to work with human language data. It provides easy-to-use interfaces, text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning"
      ],
      "metadata": {
        "id": "6PblbTdpqrZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnsTGMvbUb76",
        "outputId": "f008db21-43df-4357-81a9-1b02e944e618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and reading file (111.txt)\n",
        "Note: this file is to be uploaded, and can be found in unlabelled datasets provided in the drive link as per lms"
      ],
      "metadata": {
        "id": "85AqsE9NrAlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DPrDKWfUb79"
      },
      "outputs": [],
      "source": [
        "\n",
        "filename=\"summarization_dataset/news_articles/001.txt\"\n",
        "f = open(\"111.txt\", \"r\")#creating a file object\n",
        "text=f.read() #Read the contents of the file into text\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the content of the file that was just read\n"
      ],
      "metadata": {
        "id": "Ga5CBTg1rdqs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMOa20ZYUb7_",
        "outputId": "684baa21-a483-4d1f-dbe1-430d99f615e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WHO's Covid weapons fight still $16.8 bn short\n",
            "\n",
            "The World Health Organization's global appeal for funding for coronavirus vaccines, treatments, diagnostics and equipment is still $16.8-billion short\n",
            "-- almost half its total needs, the WHO said Tuesday. The funding shortfall comes amid a widening gap between rich and poor nations in their ability\n",
            "to fight the pandemic, with access to vaccines woefully uneven. WHO chief Tedros Adhanom Ghebreyesus, sounding an alarm on the gap in access to resources,\n",
            "warned that the pandemic remained in a \"very dangerous phase\" more than 18 months in. \n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing:\n",
        "\n",
        "\n",
        "here we convert the text in lower case and remove the stop words.\n",
        "stop words can be imported using nltk packages.\n",
        "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. "
      ],
      "metadata": {
        "id": "teX3o8_ir9nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frvj-_vUUb8B",
        "outputId": "40bc2870-1f4a-4305-ba42-99b34c273a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sent_tokens = nltk.sent_tokenize(text.lower())\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "stopWords = list(set(stopwords.words(\"english\")))\n",
        "word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "print(len(word_tokens_refined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6vYNKUJUb8C",
        "outputId": "2179210a-cbdc-47f2-d976-f283e61b6ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who's covid weapons fight still $16.8 bn short\n",
            "\n",
            "the world health organization's global appeal for funding for coronavirus vaccines, treatments, diagnostics and equipment is still $16.8-billion short\n",
            "-- almost half its total needs, the who said tuesday.\n",
            "['WHO', \"'s\", 'Covid', 'weapons', 'fight', 'still', '$', '16.8']\n",
            "['who', \"'s\", 'covid', 'weapons', 'fight', 'still', '$', '16.8']\n",
            "[\"'s\", 'covid', 'weapons', 'fight', 'still', '$', '16.8', 'bn']\n"
          ]
        }
      ],
      "source": [
        "print(sent_tokens[0])\n",
        "print(word_tokens[:8])\n",
        "print(word_tokens_lower[:8])\n",
        "print(word_tokens_refined[:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we simply count the number of times a word appears in the document, and thus formulate the Frequency Distribution."
      ],
      "metadata": {
        "id": "9thZxodEsmVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKZNxpenUb8C",
        "outputId": "8f850852-9d0f-48e7-c09a-933541cf9ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58\n"
          ]
        }
      ],
      "source": [
        "\n",
        "freqTable = dict() \n",
        "for word in word_tokens_refined: \n",
        "    if word in freqTable: \n",
        "        freqTable[word] += 1\n",
        "    else: \n",
        "        freqTable[word] = 1\n",
        "print(len(freqTable))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking reference from the frequencytable we Compute score of each sentence. This score will be used as a basis to include or not include the statement in the summary."
      ],
      "metadata": {
        "id": "GbKlLCM_s0LK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWDfFLWmUb8D"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentenceValue = dict()   \n",
        "for sentence in sent_tokens: \n",
        "    for word in nltk.word_tokenize(sentence): \n",
        "        if word in freqTable.keys():\n",
        "            if sentence in sentenceValue: \n",
        "                sentenceValue[sentence] += freqTable[word] \n",
        "            else: \n",
        "                sentenceValue[sentence] = freqTable[word]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the score for each is computed, we will find an average score.\n",
        "\n",
        "If the score is satisfacctory we add it to our summary sentence.\n",
        "\n",
        "Finally we can print this sentence and which is the final summary."
      ],
      "metadata": {
        "id": "BJEUiDU0tHrH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aswIcKOLUb8F",
        "outputId": "255af062-5e3c-40b0-c781-9b98d4bcf305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            " who's covid weapons fight still $16.8 bn short\n",
            "\n",
            "the world health organization's global appeal for funding for coronavirus vaccines, treatments, diagnostics and equipment is still $16.8-billion short\n",
            "-- almost half its total needs, the who said tuesday.\n"
          ]
        }
      ],
      "source": [
        "sumValues = 0\n",
        "for sentence in sentenceValue: \n",
        "    sumValues += sentenceValue[sentence] \n",
        "average = int(sumValues / len(sentenceValue)) \n",
        "print(average)\n",
        "# Storing sentences into our summary. \n",
        "summary = '' \n",
        "for sentence in sent_tokens: \n",
        "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.1*average)): \n",
        "        summary += \" \" + sentence \n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "A1 ELC",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}